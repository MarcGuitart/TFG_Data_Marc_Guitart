# TFG Data Pipeline â€” Week 2

## ğŸ¯ Objectiu de la setmana
Implementar un pipeline **E2E** dâ€™ingesta i processament amb **Kafka + InfluxDB**, operable via **API** i verificable amb comandes simples.

Aquesta setmana hem:
- Afegit **ingesta des de CSV/Parquet** (i mantenim el format a la sortida).
- Tancat el flux **Loader â†’ Agent â†’ InfluxDB â†’ Collector â†’ Orchestrator**.
- Exposat **health i mÃ¨triques** bÃ siques.
- Activat **autoâ€gestiÃ³** de dades (cap per unitat + purge).
- Documentat **com verificar** tot amb `curl`.

---

## ğŸ—ï¸ Arquitectura

```mermaid
flowchart LR
  subgraph Client
    U[CLI / Frontend]
  end

  subgraph Orchestrator
    O1[/POST /api/upload_csv/]
    O2[/POST /api/run_window/]
    O3[/GET  /api/flush/]
    O4[/GET  /metrics/]
    O5[/GET  /health/]
  end

  subgraph Loader
    L1[window_loader]
  end

  subgraph Agent
    A1[agent]
  end

  subgraph Collector
    C1[window_collector]
  end

  subgraph TSDB
    I[(InfluxDB 2.7)]
  end

  U --> O1
  U --> O2
  U --> O3
  U --> O4
  U --> O5

  O2 --> L1
  L1 -->|topic: telemetry.agent.in| A1
  A1 -->|write| I
  A1 -->|topic: telemetry.agent.out| C1
  C1 -->|dedupe + flush| U
```

---

## ğŸ§° Serveis

- **Zookeeper** Â· coordinaciÃ³ de Kafka  
- **Kafka** Â· bus dâ€™esdeveniments (`telemetry.agent.in`, `telemetry.agent.out`, `telemetry.processed`)  
- **InfluxDB 2.7** Â· emmagatzematge de sÃ¨ries temporals (bucket `pipeline`)  
- **window_loader** Â· ingesta de fitxers i publicaciÃ³ a Kafka  
- **agent** Â· consum de Kafka, escriptura a Influx i reâ€emissiÃ³ a Kafka  
- **window_collector** Â· dedupe (Ãºltim per clau) + `flush` a disc (CSV/Parquet)  
- **orchestrator** Â· API central (upload, trigger, flush, mÃ¨triques, health)

---

## ğŸš€ Desplegament

```bash
# ConstrucciÃ³ i arrencada
docker compose build
docker compose up -d

# Logs dâ€™un servei
docker compose logs -f agent
```

> **Ports per defecte**: Orchestrator `8081`, Collector `8082`, InfluxDB `8086`.

---

## ğŸ§ª Flux endâ€‘toâ€‘end

### 1) Pujar un CSV
```bash
curl -s -X POST http://localhost:8081/api/upload_csv \
  -F "file=@data/test_csvs/test_small.csv"
```

### 2) Executar la finestra/procÃ©s
```bash
curl -s -X POST http://localhost:8081/api/run_window | jq
```
Exemple de resposta:
```json
{
  "triggered": true,
  "status_code": 200,
  "loader_response": {
    "rows": 5,
    "path": "/app/data/processed_window.csv"
  }
}
```

### 3) Consultar el Collector (dedupe + flush)
```bash
curl -s http://localhost:8082/flush | jq
```
Exemple:
```json
{
  "rows": 5,
  "path": "/app/data/processed_window.csv"
}
```

### 4) Validar a InfluxDB (API HTTP)
> Recomanat usar rang dâ€™**Ãºltimes 24h** per a dades de test amb timestamps antics.
```bash
curl -s "http://localhost:8086/api/v2/query?org=tfg" \
  -H "Authorization: Token admin_token" \
  -H "Accept: application/csv" \
  -H "Content-Type: application/vnd.flux" \
  -d 'from(bucket:"pipeline") |> range(start:-24h) |> limit(n:10)'
```

---

## ğŸ“Š MÃ¨triques exposades (Orchestrator)

### JSON
```bash
curl -s http://localhost:8081/metrics | jq
```
Sortida tÃ­pica:
```json
{
  "uptime_sec": 120,
  "points_written": 0,
  "last_flush_rows": 5
}
```

### Prometheus
```bash
curl -s http://localhost:8081/metrics/prometheus
```
```
uptime_sec 120
points_written 0
last_flush_rows 5
```

> **Nota**: `points_written` Ã©s un placeholder; es generalitzarÃ  amb mÃ¨trica real a lâ€™Agent (veure â€œNext Weekâ€).

---

## ğŸ§¹ Autoâ€‘gestiÃ³ de dades (Agent)

- **LÃ­mit per unitat**: `MAX_ROWS_PER_UNIT` (per defecte: `1000`).  
- Si se supera, sâ€™aplica purge de punts **antics** (finestra defensiva de 7 dies).  
- Objectiu: evitar creixements descontrolats i mantenir la TSDB lleugera.

> Proper pas: alinear la polÃ­tica de purge amb `RETENTION_HOURS` o retenciÃ³ nativa del bucket (7d).

---

## âœ… Checklist Week 2

- [x] Loader amb suport **CSV i Parquet**  
- [x] Agent **escriu a InfluxDB** (measurement `telemetry`, tag `unit`)  
- [x] Collector guarda **Ãºltima versiÃ³ per clau** i fa **flush** (CSV/Parquet)  
- [x] Orchestrator amb APIs `/upload_csv`, `/run_window`, `/flush`, `/metrics`, `/health`  
- [x] MÃ¨triques bÃ siques (JSON + Prometheus)  
- [x] Autoâ€‘gestiÃ³ de dades activa (cap + purge)  
- [x] DocumentaciÃ³ per verificaciÃ³ E2E

---

## ğŸ§ª (Opcional) Smoke test de 1 minut

```bash
#!/usr/bin/env bash
set -euo pipefail

ok(){ echo "OK: $1"; }
fail(){ echo "FAIL: $1"; exit 1; }

# 1) Upload
curl -fsS -X POST http://localhost:8081/api/upload_csv \
  -F "file=@data/test_csvs/test_small.csv" >/dev/null || fail "upload"

# 2) Run
curl -fsS -X POST http://localhost:8081/api/run_window >/dev/null || fail "run_window"

# 3) Flush collector
rows=$(curl -fsS http://localhost:8082/flush | jq -r '.rows') || fail "flush"
[[ "$rows" -ge 1 ]] || fail "collector rows = $rows"
ok "collector rows = $rows"

# 4) Query Influx (24h)
q=$(curl -fsS "http://localhost:8086/api/v2/query?org=tfg" \
    -H "Authorization: Token admin_token" \
    -H "Accept: application/csv" \
    -H "Content-Type: application/vnd.flux" \
    -d 'from(bucket:"pipeline") |> range(start:-24h) |> limit(n:1)')
echo "$q" | grep -q "_measurement" || fail "influx query"
ok "influx query retornada"
```

---

## ğŸ“Œ Next Week (millores planificades)

- **RetenciÃ³ consistent**  
  - **OpciÃ³ A**: mana Influx (7d) + cap per unitat a lâ€™Agent  
  - **OpciÃ³ B**: usar `RETENTION_HOURS` a lâ€™Agent i documentar la polÃ­tica
- **MÃ¨triques reals**  
  - Exposar `agent_points_written` (Prometheus) o reportar a lâ€™Orchestrator
- **Timestamps â€œdevâ€‘friendlyâ€**  
  - Flag `FORCE_NOW=true` per veure dades amb `range(-5m)`
- **ResiliÃ¨ncia**  
  - Retries/backoff i logs dâ€™error amb context (Kafka/HTTP)
- **Docs**  
  - README ampliat + troubleshooting rÃ pid

---

## ğŸ› ï¸ Troubleshooting (rÃ pid)

- **No veus dades amb `-5m`** â†’ probablement timestamps antics al CSV. Prova `-24h` o activa `FORCE_NOW=true` a lâ€™Agent.  
- **â€œClosing connectionâ€ als logs de Kafka** â†’ Ã©s **normal** (connexiÃ³ bootstrap i desprÃ©s al lÃ­der).  
- **CLI Kafka al contenidor** â†’ pot no estar disponible; valida amb **Collector `/flush`** i la **API dâ€™Influx**.
